{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DatalakeDemo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdeyhim/google-demo/blob/master/DatalakeDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "d4tzUHQ4nfZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sPThzRYZoZNz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The goal of this demo is to showcase an actual customer use-case to leverage BigQuery as the single source of truth for streaming and batch data sources. For this use-case customerâ€™s data sources (streaming and batch) had a common schema. In order to make the pipeline simple and show the power of Dataflow, the architecture converts batch files to streaming records. The following depicts the overall architecture\n",
        "[Notebook on Github](https://github.com/pdeyhim/google-demo/blob/master/DatalakeDemo.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "QSF2hnUfh-eS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3a2JFPY3k0Dy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ]
    },
    {
      "metadata": {
        "id": "j54njH73BN88",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Before getting started lets fill in some of the variables that we need to execute some of the codes later in this notebook. The fields with \"Insert Text Here\" are required fields. **\n",
        "\n",
        "**project_name should be set to your qwiklab project name**\n",
        "\n",
        "**gcs_bucket_name a unique name for your Google Cloud Storage bucket. Please keep this short and simple**\n",
        "\n",
        "\n",
        "**[optional] dataset_name: your dataset name. Pleas keep this name short and simple**\n",
        "\n",
        "**[optional] table_name: your bigquery table name. Please keep this name short and simple**"
      ]
    },
    {
      "metadata": {
        "id": "H_RL5nMhNdsv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "project_name = ''  #@param {type: \"string\"}\n",
        "gcs_bucket_name = '' #@param {type: \"string\"}\n",
        "dataset_name = 'demo'  #@param {type: \"string\"}\n",
        "table_name = 'random_actors'  #@param {type: \"string\"}\n",
        "data_source_topic_name = table_name+\"_incoming_v1\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HdkfZ1jrMLfQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://raw.githubusercontent.com/pdeyhim/google-demo/master/images/BQColabDiagrams/arch.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "aIdE1X9PoMSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**A:** Initially  Dataflow sample data generators will generate two types of data: streaming and batch data. Streaming dataflow job publishes records to pubsub. Batch dataflow job stores batch data in GCS.\n",
        "\n",
        "**B:** Batch to streaming conversion dataflow job scans GCS for any new batch files generated by the batch dataflow job and converts each line to a record that later gets published to the streaming pubsub topic. \n",
        "\n",
        "**C:** Data ingest to BigQuery dataflow job reads data from pubsub topic that has both streaming and batch records and publishes the records to BigQuery.\n"
      ]
    },
    {
      "metadata": {
        "id": "WLYDF6AOApH9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "metadata": {
        "id": "aqyHhb0lAxiU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We wil begin with installing our dependencies"
      ]
    },
    {
      "metadata": {
        "id": "Q7jJYFVbL4--",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install google-cloud-pubsub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "shbLVnJIAC1i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get -y install jq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yp5w6gPcLaS4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import pubsub_v1\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "741sN98okuVo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "Cjk_TKkWPlqZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticate Yourself"
      ]
    },
    {
      "metadata": {
        "id": "U6QpyMCjCzQH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before executing any code we need to authenticate with Google Cloud. User the credentials you were given bu Qwiklab to authenticate. Once authenticate you'll be presented with an authentication code. You'll be asked to copy/paste the given code to the text box provided below"
      ]
    },
    {
      "metadata": {
        "id": "ldQA_rFcLeeP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2xB4zGVYklGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eC-NqIuaVtYS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create Google Cloud Resources"
      ]
    },
    {
      "metadata": {
        "id": "CMpBwB7AWdAN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need to create a few Google Cloud resources before staring our Dataflow Jobs: Google Cloud Storage bucket, PubSub topic, and BigQuery table\n"
      ]
    },
    {
      "metadata": {
        "id": "QRy-bfXdXQwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating PubSub Topic"
      ]
    },
    {
      "metadata": {
        "id": "Ie4xOW0nLUJP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "topic_name = 'projects/{project_id}/topics/{topic}'.format(\n",
        "    project_id=project_name,\n",
        "    topic=data_source_topic_name, \n",
        ")\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "\n",
        "try:\n",
        "  publisher.create_topic(topic_name)\n",
        "  print(\"Created PubSub Topic: {}\".format(data_source_topic_name))\n",
        "except Exception as e: print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kk_5JytSWwjT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're using BigQuery as our data lake. For this specific dataset we're creating a table that will hold raw data"
      ]
    },
    {
      "metadata": {
        "id": "caJlr6vXOT5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bq_client = bigquery.Client(project=project_name)\n",
        "bq_client.create_dataset(dataset=dataset_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0u01NqQDOdK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bq_client.query('''CREATE OR REPLACE TABLE `{dataset}.{table}` (ts timestamp, user_id INT64, actor STRING, location STRUCT<zip STRING, longitude\t STRING, latitude\tSTRING>, v1 FLOAT64, v2 FLOAT64, v3 FLOAT64, v4 FLOAT64, v5 FLOAT64, v6 FLOAT64) \n",
        "  PARTITION BY DATE(ts)'''.format(dataset=dataset_name,table=table_name)).result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CA4JjurWYoG2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating GCS Bucket that will hold some configurations and also incoming batch data"
      ]
    },
    {
      "metadata": {
        "id": "qNZLIpMAP20R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "storage_client = storage.Client(project=project_name)\n",
        "storage_client = storage.Client(project=project_name)\n",
        "\n",
        "try: \n",
        "  storage_client.create_bucket(gcs_bucket_name)\n",
        "  print(\"bucket created\")\n",
        "except Exception as e: print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aEio0KvgkyYg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4B0UboY3ZUqi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Generating Data\n"
      ]
    },
    {
      "metadata": {
        "id": "4Xo_d8X0Zd9R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section we'll generate random sample data in shape of json records both in streaming and batch format using Dataflow clusters.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SHzrNrVWMaqJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://raw.githubusercontent.com/pdeyhim/google-demo/master/images/BQColabDiagrams/datagen.jpg\n",
        ")"
      ]
    },
    {
      "metadata": {
        "id": "0f3rouDKY2Uh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Defining the schema of generated data for both Streaming and Batch data"
      ]
    },
    {
      "metadata": {
        "id": "Rjxt6o1bYmid",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bucket = storage_client.get_bucket(gcs_bucket_name)\n",
        "streaming_actor_schema = bucket.blob(\"datagen-schema/streaming-actor.json\")\n",
        "batch_actor_schema = bucket.blob(\"datagen-schema/batch-actor.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gd6GKLIiQRhF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "streaming_actor_schema.upload_from_string('''\n",
        "[\n",
        "  {\"name\": \"user_id\",\"class\":\"foreign-key\",\"size\": 10000,\"skew\":0},\n",
        "  {\"name\": \"actor\", \"class\":\"string\", \"dist\":{\"streaming-actor\":1}},\n",
        "  {\"name\": \"ts\", \"class\":\"current-time\"},\n",
        "  {\"name\":\"v1\", \"class\":\"int\", \"min\":1, \"max\":2},\n",
        "  {\"name\": \"v2\", \"class\": \"random-walk\"},\n",
        "  {\"name\": \"v3\", \"class\": \"random-walk\",\"s\": 1},\n",
        "  {\"name\": \"v4\", \"class\": \"random-walk\", \"precision\": {\"class\": \"gamma\", \"dof\": 1}},\n",
        "  {\"name\": \"v5\", \"class\": \"gamma\", \"alpha\": 0.1, \"beta\": 0.1},\n",
        "  {\"name\": \"v6\", \"class\": \"random-walk\", \"mean\":100, \"sd\":5},\n",
        "  {\"name\": \"location\",\"class\": \"zip\",\"near\": \"37.77,-122.41\",\"milesFrom\": 1, \"fields\":\"latitude, longitude, zip\"}\n",
        "]\n",
        "''')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJtEounKTbJq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_actor_schema.upload_from_string('''\n",
        "[\n",
        "  {\"name\": \"user_id\",\"class\":\"foreign-key\",\"size\": 10000,\"skew\":0},\n",
        "  {\"name\": \"actor\", \"class\":\"string\", \"dist\":{\"batch-actor\":1}},\n",
        "  {\"name\": \"ts\", \"class\":\"current-time\"},\n",
        "  {\"name\": \"v1\", \"class\":\"sin\"},\n",
        "  {\"name\": \"v2\", \"class\": \"random-walk\"},\n",
        "  {\"name\": \"v3\", \"class\": \"random-walk\",\"s\": 1},\n",
        "  {\"name\": \"v4\", \"class\": \"random-walk\", \"precision\": {\"class\": \"gamma\", \"dof\": 1}},\n",
        "  {\"name\": \"v5\", \"class\": \"gamma\", \"alpha\": 0.1, \"beta\": 0.1},\n",
        "  {\"name\": \"v6\", \"class\": \"random-walk\", \"mean\":100, \"sd\":5},\n",
        "  {\"name\": \"location\",\"class\": \"zip\",\"near\": \"37.77,-122.41\",\"milesFrom\": 1, \"fields\":\"latitude, longitude, zip\"}\n",
        "]\n",
        "''')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6DQm2E4ZF4r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uploading datagenerator schemas to GCS"
      ]
    },
    {
      "metadata": {
        "id": "Nx50KyNuTfzE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for blob in bucket.list_blobs(prefix=\"datagen-schema\"):\n",
        "  print(blob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8sCGyV_1pjpR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Downloading data generator code"
      ]
    },
    {
      "metadata": {
        "id": "odK9W2PjWW8E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash \n",
        "wget https://www-us.apache.org/dist/maven/maven-3/3.6.0/binaries/apache-maven-3.6.0-bin.tar.gz; tar -xzf apache-maven-3.6.0-bin.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LyoF0eZNXKz4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/pdeyhim/log-synth.git /content/log-synth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hMqb-uGZCq9y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /content/log-synth\n",
        "\n",
        "/content/apache-maven-3.6.0/bin/mvn  install -Dmaven.test.skip=true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x5QzgeeGW4tu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/pdeyhim/datagenerator.git /content/datagenerator/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7a8FTPnY0kv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/datagenerator/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JZWirVz5Z_TV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Starting streaming data generator Dataflow job"
      ]
    },
    {
      "metadata": {
        "id": "EMr62_FrRf4E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "streaming_datagen_mvn_command = '''/content/apache-maven-3.6.0/bin/mvn compile exec:java \\\n",
        " -Dexec.mainClass=com.google.datagenerator.SampleDataGeneratorStreaming \\\n",
        " -Dexec.cleanupDaemonThreads=false \\\n",
        " -Dexec.args=\" \\\n",
        " --project={project_name} \\\n",
        " --stagingLocation=gs://{gcs_bucket_name}/dataflow/pipelines/sampler/staging \\\n",
        " --tempLocation=gs://{gcs_bucket_name}/dataflow/pipelines/sampler/temp \\\n",
        " --runner=DataflowRunner \\\n",
        " --windowDuration=5s \\\n",
        " --numShards=5 \\\n",
        " --qps=100 \\\n",
        " --schemaLocation=gs://{gcs_bucket_name}/datagen-schema/streaming-actor.json \\\n",
        " --outputTopic=projects/{project_name}/topics/{data_source_topic_name} \\\n",
        " --autoscalingAlgorithm=THROUGHPUT_BASED \\\n",
        " --maxNumWorkers=3\"\n",
        "'''.format(gcs_bucket_name=gcs_bucket_name,project_name=project_name,data_source_topic_name=data_source_topic_name)\n",
        "\n",
        "out = subprocess.run(streaming_datagen_mvn_command,shell=True,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "out.stdout.decode().split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ag_G5gq4aWql",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Starting batch data generator Dataflow job"
      ]
    },
    {
      "metadata": {
        "id": "sT4OEywkftEZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_datagen_mvn_command = '''/content/apache-maven-3.6.0/bin/mvn compile exec:java \\\n",
        " -Dexec.mainClass=com.google.datagenerator.SampleDataGeneratorBatch \\\n",
        " -Dexec.cleanupDaemonThreads=false \\\n",
        " -Dexec.args=\" \\\n",
        " --project={project_name} \\\n",
        " --stagingLocation=gs://{gcs_bucket_name}/dataflow/pipelines/sampler/staging \\\n",
        " --tempLocation=gs://{gcs_bucket_name}/dataflow/pipelines/sampler/temp \\\n",
        " --runner=DataflowRunner \\\n",
        " --windowDuration=10m \\\n",
        " --numShards=5 \\\n",
        " --qps=1000 \\\n",
        " --schemaLocation=gs://{gcs_bucket_name}/datagen-schema/batch-actor.json \\\n",
        " --outputDirectory=gs://{gcs_bucket_name}/dataflow-sampler/data/ \\\n",
        " --outputFilenamePrefix=user-simulated-data- \\\n",
        " --outputFilenameSuffix=.json \\\n",
        " --autoscalingAlgorithm=THROUGHPUT_BASED \\\n",
        " --maxNumWorkers=3\"'''.format(gcs_bucket_name=gcs_bucket_name,project_name=project_name )\n",
        "\n",
        "out = subprocess.run(batch_datagen_mvn_command,shell=True,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "out.stdout.decode().split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jmhhcYzqDZMZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once the commands above have successfully executed you should be able to view the progress of each Dataflow job in the consose: https://pantheon.corp.google.com/dataflow"
      ]
    },
    {
      "metadata": {
        "id": "lTpdE0foa44O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Converting Batch to Streaming Data\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2yALsdkoMgR0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://raw.githubusercontent.com/pdeyhim/google-demo/master/images/BQColabDiagrams/batch2stream.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "9px4yapCa_TG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash \n",
        "git clone https://github.com/GoogleCloudPlatform/DataflowTemplates.git /content/DataflowTemplates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uMIAPIgAdnWj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/DataflowTemplates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-gBfWt-bUAx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " gcs_to_pubsub = '''/content/apache-maven-3.6.0/bin/mvn compile exec:java \\\n",
        "-Dexec.mainClass=com.google.cloud.teleport.templates.TextToPubsubStream \\\n",
        "-Dexec.args=\" \\\n",
        "--project={project_name} \\\n",
        "--stagingLocation=gs://{gcs_bucket_name}/dataflow/pipelines/gcstopubsub/staging \\\n",
        "--tempLocation=gs://{gcs_bucket_name}/dataflow/pipelines/gcstopubsub/temp2 \\\n",
        "--runner=DataflowRunner \\\n",
        "--inputFilePattern=gs://{gcs_bucket_name}/dataflow-sampler/data/user-simulated-data-* \\\n",
        "--outputTopic=projects/{project_name}/topics/{data_source_topic_name}\"'''.format(gcs_bucket_name=gcs_bucket_name,project_name=project_name,data_source_topic_name=data_source_topic_name)\n",
        "\n",
        "out = subprocess.run(gcs_to_pubsub,shell=True,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "out.stdout.decode().split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RIRCJ4IDqH0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Streaming Data to BigQuery"
      ]
    },
    {
      "metadata": {
        "id": "yq-iKIM4MnaQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://raw.githubusercontent.com/pdeyhim/google-demo/master/images/BQColabDiagrams/pubsubtobq.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "BL3dFWaZeMSi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once both streaming and batch data is replicated to PubSub, the final dataflow job is responsible for ensuring data is copied to our raw BigQuery"
      ]
    },
    {
      "metadata": {
        "id": "Kgyd06xaeNfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pubsub_to_bigquery= '''/content/apache-maven-3.6.0/bin/mvn compile exec:java \\\n",
        "-Dexec.mainClass=com.google.cloud.teleport.templates.PubSubToBigQuery \\\n",
        "-Dexec.args=\" \\\n",
        "--project={project_name} \\\n",
        "--stagingLocation=gs://{gcs_bucket_name}/dataflow/pipelines/pubsubtobq/staging \\\n",
        "--tempLocation=gs://{gcs_bucket_name}/dataflow/pipelines/pubsubtobq/temp2 \\\n",
        "--runner=DataflowRunner \\\n",
        "--inputTopic=projects/{project_name}/topics/{data_source_topic_name} \\\n",
        "--outputTableSpec={bigquery_tableid}\"'''.format(gcs_bucket_name=gcs_bucket_name,project_name=project_name,data_source_topic_name=data_source_topic_name,bigquery_tableid=project_name+\":\"+dataset_name+\".\"+table_name)\n",
        "\n",
        "out = subprocess.run(pubsub_to_bigquery,shell=True,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "out.stdout.decode().split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x4uGmLlGjSfl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exploring Data"
      ]
    },
    {
      "metadata": {
        "id": "-22UHv4nGaKk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "he dataflow jobs that you've just launched are inserting data into BigQuery table in real-time. Execute the command below to run a SQL query to see the data being inserted. This query simply gives us the unique values for the \"actor\" column. You should see two unique values: batch-actor and streaming-actor. This cell will continue running until two unique value is reported by BigQuery\n"
      ]
    },
    {
      "metadata": {
        "id": "bhxMNaSNCUYn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = bq_client.query(\"SELECT DISTINCT actor FROM `{dataset}.{table}`\".format(dataset=dataset_name,table=table_name)).to_dataframe()\n",
        "\n",
        "while (len(df) <= 1):\n",
        "  df = bq_client.query(\"SELECT DISTINCT actor FROM `{dataset}.{table}`\".format(dataset=dataset_name,table=table_name)).to_dataframe()\n",
        "  time.sleep(10)\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4y5FUUQOjnFF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Transformation: Aggregation Dataflow\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pLsWVw2qHQSW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we're going to use another dataflow job that will use the same pubsub topic but instead of inserting every record into BigQuery, the dataflow job will aggregate the incoming records on \"v1_total\" column. This demonstrates the ability to create a real-time aggregated column. Generally the aggregation/summary tables are created using some ETL jobs that may run in batch timeframes (i.e every 10 min) which inherently means our summary table is always stale. This job ensures data is always aggregated with the most recent data."
      ]
    },
    {
      "metadata": {
        "id": "x18TOvTfI2BA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll start by creating a summary table in BigQuery that our dataflow job will insert aggregated results to."
      ]
    },
    {
      "metadata": {
        "id": "0I0ebX2U5R9J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agg_table= table_name + \"_agg\"\n",
        "bq_client.query('''CREATE OR REPLACE TABLE `{dataset}.{agg_table}` (user_id STRING, v1_total INT64, ts TIMESTAMP) \n",
        "  PARTITION BY DATE(ts)'''.format(dataset=dataset_name,agg_table=agg_table)).result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CR_Uj7I_6LTY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd /content/datagenerator/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "drNjLpi7BaIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " realtime_aggregation_job = '''/content/apache-maven-3.6.0/bin/mvn compile exec:java \\\n",
        "-Dexec.mainClass=com.google.datagenerator.RealtimeAggregationExample \\\n",
        "-Dexec.cleanupDaemonThreads=false \\\n",
        "-Dexec.args=\" \\\n",
        "--project={project_name} \\\n",
        "--stagingLocation=gs://{gcs_bucket_name}/dataflow/pipelines/realtimeagg/staging \\\n",
        "--tempLocation=gs://{gcs_bucket_name}/dataflow/pipelines/realtimeagg/temp \\\n",
        "--runner=DataflowRunner \\\n",
        "--inputTopic=projects/{project_name}/topics/{data_source_topic_name} \\\n",
        "--bigQueryTable={dataset}.{agg_table}\"'''.format(gcs_bucket_name=gcs_bucket_name,project_name=project_name,data_source_topic_name=data_source_topic_name,dataset=dataset_name,agg_table=agg_table)\n",
        "\n",
        "out = subprocess.run(realtime_aggregation_job,shell=True,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "out.stdout.decode().split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mq7WearMInhn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets take a look at our summary table. The following cell will continue running until a few records are being reported by BigQuery"
      ]
    },
    {
      "metadata": {
        "id": "s_ljEBl9Btdr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_agg = bq_client.query(\"SELECT * FROM `{dataset}.{table}` ORDER BY v1_total DESC LIMIT 10\".format(dataset=dataset_name,table=agg_table)).to_dataframe()\n",
        "while (len(df_agg) <= 1):\n",
        "  df_agg = bq_client.query(\"SELECT * FROM `{dataset}.{table}` ORDER BY v1_total DESC LIMIT 10\".format(dataset=dataset_name,table=agg_table)).to_dataframe()\n",
        "  time.sleep(10)\n",
        "df_agg.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oUVhTcVkALWz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deleting Google Cloud Resources"
      ]
    },
    {
      "metadata": {
        "id": "fg8h-5-QNGBx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set this to YES before executing the proceeding cells"
      ]
    },
    {
      "metadata": {
        "id": "D2t2jupf4qje",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "DELETE = 'NO' #@param [\"YES\", \"NO\"] { run: \"auto\" }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S5Wv5K6NMbcs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Stopping Dataflow jobs"
      ]
    },
    {
      "metadata": {
        "id": "jQcOTUsJ7yaD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash -s \"$project_name\" \"$DELETE\"\n",
        "if [ $2 = \"YES\" ] ; then \n",
        "  for i in `gcloud --project $1 --format json dataflow jobs list --status active | jq  -r '.[] | .id'` ; do gcloud --project $1 dataflow jobs cancel $i  ; done \n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YU69s7wMMfkV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Deleting PubSub Topic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7hDK9DiwAMJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if (DELETE in \"YES\"):\n",
        "  try:\n",
        "    publisher.delete_topic(topic_name)\n",
        "  except Exception as e: print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJVcIMZaMjtV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Delete BigQuery tables and dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Culu4hJSAQmV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if (DELETE in \"YES\"):\n",
        "  try:\n",
        "    bq_client.delete_table(\"{project}.{dataset}.{table}\".format(project=project_name,dataset=dataset_name,table=table_name))\n",
        "    bq_client.delete_table(\"{project}.{dataset}.{table}\".format(project=project_name,dataset=dataset_name,table=agg_table))\n",
        "    bq_client.delete_dataset(dataset=dataset_name)\n",
        "  except Exception as e: print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZdFMtfXMoTw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Delete GCS objects and bucket"
      ]
    },
    {
      "metadata": {
        "id": "4O7sPHC4HLEv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if (DELETE in \"YES\"):\n",
        "  try:\n",
        "    print(\"Deleting blobs\")\n",
        "    for blob in bucket.list_blobs():\n",
        "      blob.delete()\n",
        "    print(\"Deleting bucket\")\n",
        "    bucket.delete(force=True)\n",
        "  except Exception as e: print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}